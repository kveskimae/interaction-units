{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper module to provide activation to network layers.\n",
    "Four types of activations with their derivates are available:\n",
    "\n",
    "- Sigmoid\n",
    "- Softmax\n",
    "- Tanh\n",
    "- ReLU\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import cPickle\n",
    "import wget\n",
    "import random\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "\n",
    "def softmax_prime(z):\n",
    "    return softmax(z) * (1 - softmax(z))\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "\n",
    "def tanh_prime(z):\n",
    "    return 1 - tanh(z) * tanh(z)\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "\n",
    "def relu_prime(z):\n",
    "    return float(z > 0)\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "    abs_path = os.path.join(os.getcwd(), 'data')\n",
    "    if not os.path.exists(abs_path):\n",
    "        os.mkdir(abs_path)\n",
    "        wget.download('http://deeplearning.net/data/mnist/mnist.pkl.gz', out='data')\n",
    "\n",
    "    print(\"load_mnist: images downloaded\")\n",
    "    data_file = gzip.open(os.path.join(os.curdir, 'data', 'mnist.pkl.gz'), 'rb')\n",
    "    training_data, validation_data, test_data = cPickle.load(data_file)\n",
    "    data_file.close()\n",
    "    print(\"load_mnist: images unpacked\")\n",
    "\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in training_data[0]]\n",
    "    training_results = [vectorized_result(y) for y in training_data[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in validation_data[0]]\n",
    "    validation_results = validation_data[1]\n",
    "    validation_data = zip(validation_inputs, validation_results)\n",
    "\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in test_data[0]]\n",
    "    test_data = zip(test_inputs, test_data[1])\n",
    "    \n",
    "    print(\"load_mnist: images split into training, validation and test sets\")\n",
    "    return training_data, validation_data, test_data\n",
    "\n",
    "\n",
    "def vectorized_result(y):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[y] = 1.0\n",
    "    return e\n",
    "\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, sizes=list(), learning_rate=1.0, mini_batch_size=16,\n",
    "                 epochs=10):\n",
    "        \"\"\"Initialize a Neural Network model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sizes : list, optional\n",
    "            A list of integers specifying number of neurns in each layer. Not\n",
    "            required if a pretrained model is used.\n",
    "\n",
    "        learning_rate : float, optional\n",
    "            Learning rate for gradient descent optimization. Defaults to 1.0\n",
    "\n",
    "        mini_batch_size : int, optional\n",
    "            Size of each mini batch of training examples as used by Stochastic\n",
    "            Gradient Descent. Denotes after how many examples the weights\n",
    "            and biases would be updated. Default size is 16.\n",
    "\n",
    "        \"\"\"\n",
    "        # Input layer is layer 0, followed by hidden layers layer 1, 2, 3...\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes)\n",
    "\n",
    "        # First term corresponds to layer 0 (input layer). No weights enter the\n",
    "        # input layer and hence self.weights[0] is redundant.\n",
    "        self.weights = [np.array([0])] + [np.random.randn(y, x) for y, x in\n",
    "                                          zip(sizes[1:], sizes[:-1])]\n",
    "\n",
    "        # Input layer does not have any biases. self.biases[0] is redundant.\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes]\n",
    "\n",
    "        # Input layer has no weights, biases associated. Hence z = wx + b is not\n",
    "        # defined for input layer. self.zs[0] is redundant.\n",
    "        self._zs = [np.zeros(bias.shape) for bias in self.biases]\n",
    "\n",
    "        # Training examples can be treated as activations coming out of input\n",
    "        # layer. Hence self.activations[0] = (training_example).\n",
    "        self._activations = [np.zeros(bias.shape) for bias in self.biases]\n",
    "\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.epochs = epochs\n",
    "        self.eta = learning_rate\n",
    "\n",
    "    def fit(self, training_data, validation_data=None):\n",
    "        \"\"\"Fit (train) the Neural Network on provided training data. Fitting is\n",
    "        carried out using Stochastic Gradient Descent Algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data : list of tuple\n",
    "            A list of tuples of numpy arrays, ordered as (image, label).\n",
    "\n",
    "        validation_data : list of tuple, optional\n",
    "            Same as `training_data`, if provided, the network will display\n",
    "            validation accuracy after each epoch.\n",
    "\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k + self.mini_batch_size] for k in\n",
    "                range(0, len(training_data), self.mini_batch_size)]\n",
    "\n",
    "            for mini_batch in mini_batches:\n",
    "                nabla_b = [np.zeros(bias.shape) for bias in self.biases]\n",
    "                nabla_w = [np.zeros(weight.shape) for weight in self.weights]\n",
    "                for x, y in mini_batch:\n",
    "                    self._forward_prop(x)\n",
    "                    delta_nabla_b, delta_nabla_w = self._back_prop(x, y)\n",
    "                    nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "                    nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "                self.weights = [\n",
    "                    w - (self.eta / self.mini_batch_size) * dw for w, dw in\n",
    "                    zip(self.weights, nabla_w)]\n",
    "                self.biases = [\n",
    "                    b - (self.eta / self.mini_batch_size) * db for b, db in\n",
    "                    zip(self.biases, nabla_b)]\n",
    "\n",
    "            if validation_data:\n",
    "                accuracy = self.validate(validation_data) / 100.0\n",
    "                print(\"Epoch {0}, accuracy {1} %.\".format(epoch + 1, accuracy))\n",
    "            else:\n",
    "                print(\"Processed epoch {0}.\".format(epoch))\n",
    "\n",
    "    def validate(self, validation_data):\n",
    "        \"\"\"Validate the Neural Network on provided validation data. It uses the\n",
    "        number of correctly predicted examples as validation accuracy metric.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        validation_data : list of tuple\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of correctly predicted images.\n",
    "\n",
    "        \"\"\"\n",
    "        validation_results = [(self.predict(x) == y) for x, y in validation_data]\n",
    "        return sum(result for result in validation_results)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the label of a single test example (image).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy.array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Predicted label of example (image).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self._forward_prop(x)\n",
    "        return np.argmax(self._activations[-1])\n",
    "\n",
    "    def _forward_prop(self, x):\n",
    "        self._activations[0] = x\n",
    "        for i in range(1, self.num_layers):\n",
    "            self._zs[i] = (\n",
    "                 self.weights[i].dot(self._activations[i - 1]) + self.biases[i]\n",
    "            )\n",
    "            self._activations[i] = sigmoid(self._zs[i])\n",
    "\n",
    "    def _back_prop(self, x, y):\n",
    "        nabla_b = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        nabla_w = [np.zeros(weight.shape) for weight in self.weights]\n",
    "\n",
    "        error = (self._activations[-1] - y) * sigmoid_prime(self._zs[-1])\n",
    "        nabla_b[-1] = error\n",
    "        nabla_w[-1] = error.dot(self._activations[-2].transpose())\n",
    "\n",
    "        for l in range(self.num_layers - 2, 0, -1):\n",
    "            error = np.multiply(\n",
    "                self.weights[l + 1].transpose().dot(error),\n",
    "                sigmoid_prime(self._zs[l])\n",
    "            )\n",
    "            nabla_b[l] = error\n",
    "            nabla_w[l] = error.dot(self._activations[l - 1].transpose())\n",
    "\n",
    "        return nabla_b, nabla_w\n",
    "\n",
    "    def load(self, filename='model.npz'):\n",
    "        \"\"\"Prepare a neural network from a compressed binary containing weights\n",
    "        and biases arrays. Size of layers are derived from dimensions of\n",
    "        numpy arrays.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str, optional\n",
    "            Name of the ``.npz`` compressed binary in models directory.\n",
    "\n",
    "        \"\"\"\n",
    "        npz_members = np.load(os.path.join(os.curdir, 'models', filename))\n",
    "\n",
    "        self.weights = list(npz_members['weights'])\n",
    "        self.biases = list(npz_members['biases'])\n",
    "\n",
    "        # Bias vectors of each layer has same length as the number of neurons\n",
    "        # in that layer. So we can build `sizes` through biases vectors.\n",
    "        self.sizes = [b.shape[0] for b in self.biases]\n",
    "        self.num_layers = len(self.sizes)\n",
    "\n",
    "        # These are declared as per desired shape.\n",
    "        self._zs = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        self._activations = [np.zeros(bias.shape) for bias in self.biases]\n",
    "\n",
    "        # Other hyperparameters are set as specified in model. These were cast\n",
    "        # to numpy arrays for saving in the compressed binary.\n",
    "        self.mini_batch_size = int(npz_members['mini_batch_size'])\n",
    "        self.epochs = int(npz_members['epochs'])\n",
    "        self.eta = float(npz_members['eta'])\n",
    "\n",
    "    def save(self, filename='model.npz'):\n",
    "        \"\"\"Save weights, biases and hyperparameters of neural network to a\n",
    "        compressed binary. This ``.npz`` binary is saved in 'models' directory.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str, optional\n",
    "            Name of the ``.npz`` compressed binary in to be saved.\n",
    "\n",
    "        \"\"\"\n",
    "        np.savez_compressed(\n",
    "            file=os.path.join(os.curdir, 'models', filename),\n",
    "            weights=self.weights,\n",
    "            biases=self.biases,\n",
    "            mini_batch_size=self.mini_batch_size,\n",
    "            epochs=self.epochs,\n",
    "            eta=self.eta\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_mnist: images downloaded\n",
      "load_mnist: images unpacked\n",
      "load_mnist: images split into training, validation and test sets\n"
     ]
    }
   ],
   "source": [
    "training_data, validation_data, test_data = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, accuracy 89.91 %.\n",
      "Epoch 2, accuracy 91.57 %.\n",
      "Epoch 3, accuracy 92.84 %.\n",
      "Epoch 4, accuracy 92.63 %.\n",
      "Epoch 5, accuracy 93.08 %.\n",
      "Epoch 6, accuracy 93.54 %.\n",
      "Epoch 7, accuracy 93.51 %.\n",
      "Epoch 8, accuracy 93.94 %.\n",
      "Epoch 9, accuracy 94.07 %.\n",
      "Epoch 10, accuracy 94.5 %.\n",
      "Epoch 11, accuracy 93.96 %.\n",
      "Epoch 12, accuracy 94.17 %.\n",
      "Epoch 13, accuracy 94.07 %.\n",
      "Epoch 14, accuracy 94.25 %.\n",
      "Epoch 15, accuracy 94.27 %.\n",
      "Epoch 16, accuracy 94.02 %.\n",
      "Epoch 17, accuracy 94.14 %.\n",
      "Epoch 18, accuracy 94.5 %.\n",
      "Epoch 19, accuracy 94.47 %.\n",
      "Epoch 20, accuracy 94.46 %.\n"
     ]
    }
   ],
   "source": [
    "reference_net = NeuralNetwork(sizes=[784, 20, 20, 10], learning_rate=3.0, mini_batch_size=10, epochs=20)\n",
    "\n",
    "reference_net.fit(training_data, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 93.87 %.\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = reference_net.validate(test_data) / 100.0\n",
    "print(\"Test accuracy {0} %.\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
