{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper module to provide activation to network layers.\n",
    "Four types of activations with their derivates are available:\n",
    "\n",
    "- Sigmoid\n",
    "- Softmax\n",
    "- Tanh\n",
    "- ReLU\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import cPickle\n",
    "import wget\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "\n",
    "def softmax_prime(z):\n",
    "    return softmax(z) * (1 - softmax(z))\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "\n",
    "def tanh_prime(z):\n",
    "    return 1 - tanh(z) * tanh(z)\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "\n",
    "def relu_prime(z):\n",
    "    return float(z > 0)\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "    global delete_indices\n",
    "    \n",
    "    abs_path = os.path.join(os.getcwd(), 'data')\n",
    "    if not os.path.exists(abs_path):\n",
    "        os.mkdir(abs_path)\n",
    "        wget.download('http://deeplearning.net/data/mnist/mnist.pkl.gz', out='data')\n",
    "\n",
    "    print(\"load_mnist: images downloaded\")\n",
    "    data_file = gzip.open(os.path.join(os.curdir, 'data', 'mnist.pkl.gz'), 'rb')\n",
    "    training_data, validation_data, test_data = cPickle.load(data_file)\n",
    "    data_file.close()\n",
    "    print(\"load_mnist: images unpacked\")\n",
    "\n",
    "    training_inputs = []\n",
    "        \n",
    "    training_points_counter = 0\n",
    "    for x in training_data[0]:\n",
    "        intermediate_pixels = np.multiply(np.repeat(x,len(x)), np.tile(x,len(x)))\n",
    "        intermediate_pixels = np.delete(intermediate_pixels, delete_indices)\n",
    "        intermediate_pixels = np.reshape(intermediate_pixels, (306936, 1))\n",
    "        training_inputs.append(intermediate_pixels)\n",
    "        training_points_counter = training_points_counter + 1\n",
    "        if training_points_counter % 500 == 0:\n",
    "            print(\"Processed training point {0} of {1}.\".format(training_points_counter, len(training_data[0])))\n",
    "        \n",
    "    #training_inputs = [np.reshape(x, (306936, 1)) for x in training_inputs] - need to do earlier, this crashes kernel\n",
    "    training_results = [vectorized_result(y) for y in training_data[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    \n",
    "    print(\"load_mnist: training data ready\")\n",
    "    \n",
    "    validation_inputs = [np.multiply(np.repeat(x,len(x)), np.tile(x,len(x))) for x in validation_data[0]]\n",
    "    validation_inputs = [np.delete(x, delete_indices) for x in validation_inputs]\n",
    "    validation_inputs = [np.reshape(x, (306936, 1)) for x in validation_inputs]\n",
    "    validation_results = validation_data[1]\n",
    "    validation_data = zip(validation_inputs, validation_results)\n",
    "    \n",
    "    print(\"load_mnist: validation data ready\")\n",
    "    \n",
    "    test_inputs = [np.multiply(np.repeat(x,len(x)), np.tile(x,len(x))) for x in test_data[0]]\n",
    "    test_inputs = [np.delete(x, delete_indices) for x in test_inputs]\n",
    "    test_inputs = [np.reshape(x, (306936, 1)) for x in test_inputs]\n",
    "    test_data = zip(test_inputs, test_data[1])\n",
    "    \n",
    "    print(\"load_mnist: test data ready\")\n",
    "    \n",
    "    print(\"load_mnist: images split into training, validation and test sets\")\n",
    "    \n",
    "    return training_data, validation_data, test_data\n",
    "\n",
    "\n",
    "def vectorized_result(y):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[y] = 1.0\n",
    "    return e\n",
    "\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, sizes=list(), learning_rate=1.0, mini_batch_size=16,\n",
    "                 epochs=10):\n",
    "        \"\"\"Initialize a Neural Network model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sizes : list, optional\n",
    "            A list of integers specifying number of neurns in each layer. Not\n",
    "            required if a pretrained model is used.\n",
    "\n",
    "        learning_rate : float, optional\n",
    "            Learning rate for gradient descent optimization. Defaults to 1.0\n",
    "\n",
    "        mini_batch_size : int, optional\n",
    "            Size of each mini batch of training examples as used by Stochastic\n",
    "            Gradient Descent. Denotes after how many examples the weights\n",
    "            and biases would be updated. Default size is 16.\n",
    "\n",
    "        \"\"\"\n",
    "        # Input layer is layer 0, followed by hidden layers layer 1, 2, 3...\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes)\n",
    "\n",
    "        # First term corresponds to layer 0 (input layer). No weights enter the\n",
    "        # input layer and hence self.weights[0] is redundant.\n",
    "        self.weights = [np.array([0])] + [np.random.randn(y, x) for y, x in\n",
    "                                          zip(sizes[1:], sizes[:-1])]\n",
    "\n",
    "        # Input layer does not have any biases. self.biases[0] is redundant.\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes]\n",
    "\n",
    "        # Input layer has no weights, biases associated. Hence z = wx + b is not\n",
    "        # defined for input layer. self.zs[0] is redundant.\n",
    "        self._zs = [np.zeros(bias.shape) for bias in self.biases]\n",
    "\n",
    "        # Training examples can be treated as activations coming out of input\n",
    "        # layer. Hence self.activations[0] = (training_example).\n",
    "        self._activations = [np.zeros(bias.shape) for bias in self.biases]\n",
    "\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.epochs = epochs\n",
    "        self.eta = learning_rate\n",
    "\n",
    "    def fit(self, training_data, validation_data=None):\n",
    "        \"\"\"Fit (train) the Neural Network on provided training data. Fitting is\n",
    "        carried out using Stochastic Gradient Descent Algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data : list of tuple\n",
    "            A list of tuples of numpy arrays, ordered as (image, label).\n",
    "\n",
    "        validation_data : list of tuple, optional\n",
    "            Same as `training_data`, if provided, the network will display\n",
    "            validation accuracy after each epoch.\n",
    "\n",
    "        \"\"\"\n",
    "        epoch_counter = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_counter = epoch_counter + 1\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k + self.mini_batch_size] for k in\n",
    "                range(0, len(training_data), self.mini_batch_size)]\n",
    "\n",
    "            counter = 0\n",
    "            for mini_batch in mini_batches:\n",
    "                if counter % 100 == 0:\n",
    "                    print(\"batch \", counter, \" of \", len(mini_batches), \" / epoch \", epoch_counter)\n",
    "                counter = counter + 1\n",
    "                nabla_b = [np.zeros(bias.shape) for bias in self.biases]\n",
    "                nabla_w = [np.zeros(weight.shape) for weight in self.weights]\n",
    "                for x, y in mini_batch:\n",
    "                    self._forward_prop(x)\n",
    "                    delta_nabla_b, delta_nabla_w = self._back_prop(x, y)\n",
    "                    nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "                    nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "                self.weights = [\n",
    "                    w - (self.eta / self.mini_batch_size) * dw for w, dw in\n",
    "                    zip(self.weights, nabla_w)]\n",
    "                self.biases = [\n",
    "                    b - (self.eta / self.mini_batch_size) * db for b, db in\n",
    "                    zip(self.biases, nabla_b)]\n",
    "\n",
    "            if validation_data:\n",
    "                accuracy = self.validate(validation_data) / 100.0\n",
    "                print(\"Epoch {0}, accuracy {1} %.\".format(epoch + 1, accuracy))\n",
    "            else:\n",
    "                print(\"Processed epoch {0}.\".format(epoch))\n",
    "\n",
    "    def validate(self, validation_data):\n",
    "        \"\"\"Validate the Neural Network on provided validation data. It uses the\n",
    "        number of correctly predicted examples as validation accuracy metric.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        validation_data : list of tuple\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of correctly predicted images.\n",
    "\n",
    "        \"\"\"\n",
    "        validation_results = [(self.predict(x) == y) for x, y in validation_data]\n",
    "        return sum(result for result in validation_results)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the label of a single test example (image).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy.array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Predicted label of example (image).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self._forward_prop(x)\n",
    "        return np.argmax(self._activations[-1])\n",
    "\n",
    "    def _forward_prop(self, x):\n",
    "        self._activations[0] = x\n",
    "        for i in range(1, self.num_layers):\n",
    "            self._zs[i] = (\n",
    "                 self.weights[i].dot(self._activations[i - 1]) + self.biases[i]\n",
    "            )\n",
    "            self._activations[i] = sigmoid(self._zs[i])\n",
    "\n",
    "    def _back_prop(self, x, y):\n",
    "        nabla_b = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        nabla_w = [np.zeros(weight.shape) for weight in self.weights]\n",
    "\n",
    "        error = (self._activations[-1] - y) * sigmoid_prime(self._zs[-1])\n",
    "        nabla_b[-1] = error\n",
    "        nabla_w[-1] = error.dot(self._activations[-2].transpose())\n",
    "\n",
    "        for l in range(self.num_layers - 2, 0, -1):\n",
    "            error = np.multiply(\n",
    "                self.weights[l + 1].transpose().dot(error),\n",
    "                sigmoid_prime(self._zs[l])\n",
    "            )\n",
    "            nabla_b[l] = error\n",
    "            nabla_w[l] = error.dot(self._activations[l - 1].transpose())\n",
    "\n",
    "        return nabla_b, nabla_w\n",
    "\n",
    "    def load(self, filename='model.npz'):\n",
    "        \"\"\"Prepare a neural network from a compressed binary containing weights\n",
    "        and biases arrays. Size of layers are derived from dimensions of\n",
    "        numpy arrays.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str, optional\n",
    "            Name of the ``.npz`` compressed binary in models directory.\n",
    "\n",
    "        \"\"\"\n",
    "        npz_members = np.load(os.path.join(os.curdir, 'models', filename))\n",
    "\n",
    "        self.weights = list(npz_members['weights'])\n",
    "        self.biases = list(npz_members['biases'])\n",
    "\n",
    "        # Bias vectors of each layer has same length as the number of neurons\n",
    "        # in that layer. So we can build `sizes` through biases vectors.\n",
    "        self.sizes = [b.shape[0] for b in self.biases]\n",
    "        self.num_layers = len(self.sizes)\n",
    "\n",
    "        # These are declared as per desired shape.\n",
    "        self._zs = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        self._activations = [np.zeros(bias.shape) for bias in self.biases]\n",
    "\n",
    "        # Other hyperparameters are set as specified in model. These were cast\n",
    "        # to numpy arrays for saving in the compressed binary.\n",
    "        self.mini_batch_size = int(npz_members['mini_batch_size'])\n",
    "        self.epochs = int(npz_members['epochs'])\n",
    "        self.eta = float(npz_members['eta'])\n",
    "\n",
    "    def save(self, filename='model.npz'):\n",
    "        \"\"\"Save weights, biases and hyperparameters of neural network to a\n",
    "        compressed binary. This ``.npz`` binary is saved in 'models' directory.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str, optional\n",
    "            Name of the ``.npz`` compressed binary in to be saved.\n",
    "\n",
    "        \"\"\"\n",
    "        np.savez_compressed(\n",
    "            file=os.path.join(os.curdir, 'models', filename),\n",
    "            weights=self.weights,\n",
    "            biases=self.biases,\n",
    "            mini_batch_size=self.mini_batch_size,\n",
    "            epochs=self.epochs,\n",
    "            eta=self.eta\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_mnist: Initialized delete indices\n"
     ]
    }
   ],
   "source": [
    "delete_indices = None\n",
    "\n",
    "num_of_pixels = 784 # len(training_data[0][0]) data is not loaded yet!\n",
    "    \n",
    "if delete_indices is None:\n",
    "    delete_indices = np.array([])\n",
    "    for pixel_idx in range(0,num_of_pixels):\n",
    "        for j in range(0,num_of_pixels+1):\n",
    "            if j <= pixel_idx:\n",
    "                idx = num_of_pixels * pixel_idx + j\n",
    "                delete_indices = np.append(delete_indices, idx)\n",
    "    print(\"load_mnist: Initialized delete indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_mnist: images downloaded\n",
      "load_mnist: images unpacked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristjanveskimae/anaconda3/envs/python27/lib/python2.7/site-packages/ipykernel_launcher.py:71: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed training point 500 of 50000.\n",
      "Processed training point 1000 of 50000.\n",
      "Processed training point 1500 of 50000.\n",
      "Processed training point 2000 of 50000.\n",
      "Processed training point 2500 of 50000.\n",
      "Processed training point 3000 of 50000.\n",
      "Processed training point 3500 of 50000.\n",
      "Processed training point 4000 of 50000.\n",
      "Processed training point 4500 of 50000.\n",
      "Processed training point 5000 of 50000.\n",
      "Processed training point 5500 of 50000.\n",
      "Processed training point 6000 of 50000.\n",
      "Processed training point 6500 of 50000.\n",
      "Processed training point 7000 of 50000.\n",
      "Processed training point 7500 of 50000.\n",
      "Processed training point 8000 of 50000.\n",
      "Processed training point 8500 of 50000.\n",
      "Processed training point 9000 of 50000.\n",
      "Processed training point 9500 of 50000.\n",
      "Processed training point 10000 of 50000.\n",
      "Processed training point 10500 of 50000.\n",
      "Processed training point 11000 of 50000.\n",
      "Processed training point 11500 of 50000.\n",
      "Processed training point 12000 of 50000.\n",
      "Processed training point 12500 of 50000.\n",
      "Processed training point 13000 of 50000.\n",
      "Processed training point 13500 of 50000.\n",
      "Processed training point 14000 of 50000.\n",
      "Processed training point 14500 of 50000.\n",
      "Processed training point 15000 of 50000.\n",
      "Processed training point 15500 of 50000.\n",
      "Processed training point 16000 of 50000.\n",
      "Processed training point 16500 of 50000.\n",
      "Processed training point 17000 of 50000.\n",
      "Processed training point 17500 of 50000.\n",
      "Processed training point 18000 of 50000.\n",
      "Processed training point 18500 of 50000.\n",
      "Processed training point 19000 of 50000.\n",
      "Processed training point 19500 of 50000.\n",
      "Processed training point 20000 of 50000.\n",
      "Processed training point 20500 of 50000.\n",
      "Processed training point 21000 of 50000.\n",
      "Processed training point 21500 of 50000.\n",
      "Processed training point 22000 of 50000.\n",
      "Processed training point 22500 of 50000.\n",
      "Processed training point 23000 of 50000.\n",
      "Processed training point 23500 of 50000.\n",
      "Processed training point 24000 of 50000.\n",
      "Processed training point 24500 of 50000.\n",
      "Processed training point 25000 of 50000.\n",
      "Processed training point 25500 of 50000.\n",
      "Processed training point 26000 of 50000.\n",
      "Processed training point 26500 of 50000.\n",
      "Processed training point 27000 of 50000.\n",
      "Processed training point 27500 of 50000.\n",
      "Processed training point 28000 of 50000.\n",
      "Processed training point 28500 of 50000.\n",
      "Processed training point 29000 of 50000.\n",
      "Processed training point 29500 of 50000.\n",
      "Processed training point 30000 of 50000.\n",
      "Processed training point 30500 of 50000.\n",
      "Processed training point 31000 of 50000.\n",
      "Processed training point 31500 of 50000.\n",
      "Processed training point 32000 of 50000.\n",
      "Processed training point 32500 of 50000.\n",
      "Processed training point 33000 of 50000.\n",
      "Processed training point 33500 of 50000.\n",
      "Processed training point 34000 of 50000.\n",
      "Processed training point 34500 of 50000.\n",
      "Processed training point 35000 of 50000.\n",
      "Processed training point 35500 of 50000.\n",
      "Processed training point 36000 of 50000.\n",
      "Processed training point 36500 of 50000.\n",
      "Processed training point 37000 of 50000.\n",
      "Processed training point 37500 of 50000.\n",
      "Processed training point 38000 of 50000.\n",
      "Processed training point 38500 of 50000.\n",
      "Processed training point 39000 of 50000.\n",
      "Processed training point 39500 of 50000.\n",
      "Processed training point 40000 of 50000.\n",
      "Processed training point 40500 of 50000.\n",
      "Processed training point 41000 of 50000.\n",
      "Processed training point 41500 of 50000.\n",
      "Processed training point 42000 of 50000.\n",
      "Processed training point 42500 of 50000.\n",
      "Processed training point 43000 of 50000.\n",
      "Processed training point 43500 of 50000.\n",
      "Processed training point 44000 of 50000.\n",
      "Processed training point 44500 of 50000.\n",
      "Processed training point 45000 of 50000.\n",
      "Processed training point 45500 of 50000.\n",
      "Processed training point 46000 of 50000.\n",
      "Processed training point 46500 of 50000.\n",
      "Processed training point 47000 of 50000.\n",
      "Processed training point 47500 of 50000.\n",
      "Processed training point 48000 of 50000.\n",
      "Processed training point 48500 of 50000.\n",
      "Processed training point 49000 of 50000.\n",
      "Processed training point 49500 of 50000.\n",
      "Processed training point 50000 of 50000.\n",
      "load_mnist: training data ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristjanveskimae/anaconda3/envs/python27/lib/python2.7/site-packages/ipykernel_launcher.py:85: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_mnist: validation data ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristjanveskimae/anaconda3/envs/python27/lib/python2.7/site-packages/ipykernel_launcher.py:93: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_mnist: test data ready\n",
      "load_mnist: images split into training, validation and test sets\n"
     ]
    }
   ],
   "source": [
    "training_data, validation_data, test_data = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Should get 50000 10000 10000\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(validation_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For safety in resource-heavy training, repeat fitting in next cell four times and save intermediate models below\n",
    "\n",
    "higher_order_net = NeuralNetwork(sizes=[306936, 20, 20, 10], learning_rate=5.0, mini_batch_size=10, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('batch ', 0, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 100, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 200, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 300, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 400, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 500, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 600, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 700, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 800, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 900, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 1000, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 1100, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 1200, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 1300, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 1400, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 1500, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 1600, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 1700, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 1800, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 1900, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 2000, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 2100, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 2200, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 2300, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 2400, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 2500, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 2600, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 2700, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 2800, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 2900, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 3000, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 3100, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 3200, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 3300, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 3400, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 3500, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 3600, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 3700, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 3800, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 3900, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 4000, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 4100, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 4200, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 4300, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 4400, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 4500, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 4600, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 4700, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 4800, ' of ', 5000, ' / epoch ', 1)\n",
      "('batch ', 4900, ' of ', 5000, ' / epoch ', 1)\n",
      "Epoch 1, accuracy 92.71 %.\n",
      "('batch ', 0, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 100, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 200, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 300, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 400, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 500, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 600, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 700, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 800, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 900, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 1000, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 1100, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 1200, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 1300, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 1400, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 1500, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 1600, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 1700, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 1800, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 1900, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 2000, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 2100, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 2200, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 2300, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 2400, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 2500, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 2600, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 2700, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 2800, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 2900, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 3000, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 3100, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 3200, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 3300, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 3400, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 3500, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 3600, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 3700, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 3800, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 3900, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 4000, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 4100, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 4200, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 4300, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 4400, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 4500, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 4600, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 4700, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 4800, ' of ', 5000, ' / epoch ', 2)\n",
      "('batch ', 4900, ' of ', 5000, ' / epoch ', 2)\n",
      "Epoch 2, accuracy 92.61 %.\n",
      "('batch ', 0, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 100, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 200, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 300, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 400, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 500, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 600, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 700, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 800, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 900, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 1000, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 1100, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 1200, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 1300, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 1400, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 1500, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 1600, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 1700, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 1800, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 1900, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 2000, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 2100, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 2200, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 2300, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 2400, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 2500, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 2600, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 2700, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 2800, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 2900, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 3000, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 3100, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 3200, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 3300, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 3400, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 3500, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 3600, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 3700, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 3800, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 3900, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 4000, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 4100, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 4200, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 4300, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 4400, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 4500, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 4600, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 4700, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 4800, ' of ', 5000, ' / epoch ', 3)\n",
      "('batch ', 4900, ' of ', 5000, ' / epoch ', 3)\n",
      "Epoch 3, accuracy 93.2 %.\n",
      "('batch ', 0, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 100, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 200, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 300, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 400, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 500, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 600, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 700, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 800, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 900, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 1000, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 1100, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 1200, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 1300, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 1400, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 1500, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 1600, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 1700, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 1800, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 1900, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 2000, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 2100, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 2200, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 2300, ' of ', 5000, ' / epoch ', 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('batch ', 2400, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 2500, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 2600, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 2700, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 2800, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 2900, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 3000, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 3100, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 3200, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 3300, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 3400, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 3500, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 3600, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 3700, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 3800, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 3900, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 4000, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 4100, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 4200, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 4300, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 4400, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 4500, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 4600, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 4700, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 4800, ' of ', 5000, ' / epoch ', 4)\n",
      "('batch ', 4900, ' of ', 5000, ' / epoch ', 4)\n",
      "Epoch 4, accuracy 93.48 %.\n",
      "('batch ', 0, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 100, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 200, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 300, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 400, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 500, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 600, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 700, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 800, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 900, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 1000, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 1100, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 1200, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 1300, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 1400, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 1500, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 1600, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 1700, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 1800, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 1900, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 2000, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 2100, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 2200, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 2300, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 2400, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 2500, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 2600, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 2700, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 2800, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 2900, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 3000, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 3100, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 3200, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 3300, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 3400, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 3500, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 3600, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 3700, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 3800, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 3900, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 4000, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 4100, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 4200, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 4300, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 4400, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 4500, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 4600, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 4700, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 4800, ' of ', 5000, ' / epoch ', 5)\n",
      "('batch ', 4900, ' of ', 5000, ' / epoch ', 5)\n",
      "Epoch 5, accuracy 93.07 %.\n",
      "('time: ', 16005.188923835754)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "higher_order_net.fit(training_data, validation_data=validation_data)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_order_net.save(filename='model_all_pairwise_20epochs.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 92.48 %.\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = higher_order_net.validate(test_data) / 100.0\n",
    "print(\"Test accuracy {0} %.\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
